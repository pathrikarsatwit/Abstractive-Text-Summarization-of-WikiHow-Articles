# -*- coding: utf-8 -*-
"""NLPFinal.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PmYrO6QcIP4OwA0RQ-QXyO1rGCkgR-Th

# Data Cleaning
"""

from datasets import load_dataset

ds = load_dataset("gursi26/wikihow-cleaned")
print(ds)

"""# Feature Engineering

"""

input = ds["train"]["text"]
target = ds["train"]["summary"]

import pandas as pd
from sklearn.model_selection import train_test_split
X = list(input)
y = list(target)
X_main, X_test, y_main, y_test = train_test_split(X, y, test_size=0.15, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_main, y_main, test_size=(0.15 / (1 - 0.15)), random_state=42)
print(f"Main set size: X: {len(X_main)}, Y: {len(y_main)}")
print(f"Training set size: X: {len(X_train)} , Y: {len(y_train)}")
print(f"Validation set size: {len(X_val)}, Y: {len(y_val)}")
print(f"Test set size: {len(X_test)}, Y: {len(y_test)}")
print(type(X_train))

X_train = [str(x) for x in X_train]
X_val = [str(x) for x in X_val]
X_test = [str(x) for x in X_test]

y_train = [str(y) for y in y_train]
y_val = [str(y) for y in y_val]
y_test = [str(y) for y in y_test]

from transformers import T5Tokenizer

# Load T5-small tokenizer
tokenizer = T5Tokenizer.from_pretrained("t5-small")

# Add "summarize: " prefix to inputs
def tokenize_batch(texts, summaries, max_input_length=256, max_output_length=128):
    texts = ["summarize: " + t for t in texts]

    model_inputs = tokenizer(
        texts,
        max_length=max_input_length,
        truncation=True,
        padding="max_length"
    )

    labels = tokenizer(
        summaries,
        max_length=max_output_length,
        truncation=True,
        padding="max_length"
    )

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# Tokenize datasets
train_enc = tokenize_batch(X_train, y_train)
val_enc   = tokenize_batch(X_val, y_val)
test_enc  = tokenize_batch(X_test, y_test)

import torch
from torch.utils.data import Dataset

class T5Dataset(Dataset):
    def __init__(self, encodings):
        self.encodings = encodings

    def __len__(self):
        return len(self.encodings["input_ids"])

    def __getitem__(self, idx):
        return {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}

# Create dataset objects
train_dataset = T5Dataset(train_enc)
val_dataset   = T5Dataset(val_enc)
test_dataset  = T5Dataset(test_enc)

"""# Model Selection"""

from transformers import T5ForConditionalGeneration

# Load T5-small model
model = T5ForConditionalGeneration.from_pretrained("t5-small")

"""# Model Training"""

# from transformers import Trainer, TrainingArguments

# training_args = TrainingArguments(
#     output_dir="./t5-small-finetuned",
#     per_device_train_batch_size=8,   # can adjust based on GPU memory
#     per_device_eval_batch_size=8,
#     num_train_epochs=2,
#     eval_strategy="epoch",
#     save_strategy="epoch",
#     logging_steps=50,
#     fp16=True                       # use mixed precision for speed
#     # Removed predict_with_generate=True as it is causing a TypeError
# )

# trainer = Trainer(
#     model=model,
#     args=training_args,
#     train_dataset=train_dataset,
#     eval_dataset=val_dataset,
#     tokenizer=tokenizer
# )
# trainer.train()

# model.save_pretrained("t5-finetuned-xsum")
# tokenizer.save_pretrained("t5Q267-finetuned-xsum")

# !pip install huggingface_hub

# from huggingface_hub import login
# login()

# # Push the model
# model.push_to_hub("Sakshi-1234/NLPFinal")
# # Push the tokenizer
# tokenizer.push_to_hub("Sakshi-1234/NLPFinal")

"""# Inference Pipeline"""

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained("Sakshi-1234/NLPFinal")
model = AutoModelForSeq2SeqLM.from_pretrained("Sakshi-1234/NLPFinal")

def generate_summary(text, max_input_length=512, max_summary_length=128, num_beams=4):
    """
    Generates a summary for a single text input.
    """
    # Tokenize input
    inputs = tokenizer(
        text,
        max_length=max_input_length,
        truncation=True,
        return_tensors="pt"
    )

    # Generate summary tokens
    summary_ids = model.generate(
      input_ids=inputs["input_ids"],
      attention_mask=inputs["attention_mask"],
      max_length=128,
      num_beams=6,
      early_stopping=True,
      repetition_penalty=2.0,
      length_penalty=1.0,
      no_repeat_ngram_size = 3,
    )


    # Decode tokens to string
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    return summary

# Example usage
article = X_test[90]
print("article: ", article)
summary = generate_summary(article)
print("Summary:", summary)

"""# Postprocessing"""

def clean_summary(summary_text):
    # Strip leading/trailing whitespace
    summary_text = summary_text.strip()

    # Optional: replace multiple spaces/newlines with a single space
    summary_text = ' '.join(summary_text.split())

    # Optional: additional cleaning logic
    # e.g., remove unwanted characters, fix punctuation

    return summary_text

# Apply postprocessing
cleaned_summary = clean_summary(summary)
print("Cleaned Summary:", cleaned_summary)

"""# Evaluation"""

!pip install rouge_score
!pip install evaluate

import evaluate

# Load ROUGE metric
rouge = evaluate.load("rouge")

def generate_summary(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding="max_length", max_length=512)

    summary_ids = model.generate(
        input_ids=inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_length=128,
        num_beams=4
    )

    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)

# Compute predictions
preds = [generate_summary(t) for t in X_test[:100]]   # limit to 100 for speed
refs  = [s for s in y_test[:100]]

# Evaluate
results = rouge.compute(predictions=preds, references=refs)
print(results)